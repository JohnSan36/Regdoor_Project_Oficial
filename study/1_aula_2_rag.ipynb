{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminhos = [\n",
    "    \"../pdfes/Explorando a API da OpenAI.pdf\",\n",
    "    \"../pdfes/Explorando o Universo das IAs com Hugging Face.pdf\",\n",
    "    \"../pdfes/Miyamoto Musashi - Gorin no Sho - O Livro dos Cinco Elementos.pdf\",\n",
    "    \"../pdfes/ultra-aprendizado-ultralearning-scott-young.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas = []\n",
    "for pdf in caminhos:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    paginas.extend(loader.load_and_split())\n",
    "\n",
    "print(len(paginas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    separators=[\".\", \"!\",\"\\n\\n\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(paginas)\n",
    "\n",
    "def limpa_texto(texto):\n",
    "    return texto.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk.page_content = limpa_texto(chunk.page_content)\n",
    "\n",
    "print(chunks[500].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(\n",
    "    chunks,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "retriever.invoke(\"Como ser um samurai?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Você é um assistente de IA que responde perguntas de acordo com o contexto fornecido.\n",
    "\n",
    "    Contexto: {contexto}\n",
    "    Pergunta: {pergunta}\n",
    "    \"\"\"\n",
    ")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup_and_retriever = RunnableParallel({\n",
    "    \"pergunta\": RunnablePassthrough(),\n",
    "    \"contexto\": retriever\n",
    "})\n",
    "\n",
    "chain = setup_and_retriever | template | model | output_parser\n",
    "\n",
    "chain.invoke(\"Como ser um samurai?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entendendo a Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunnablePassthrough().invoke(\"Como ser um samurai?\") # Passa a pergunta para o modelo\n",
    "\n",
    "retriever.invoke(\"Como ser um samurai?\") # Passa a pergunta para o retriever\n",
    "\n",
    "RunnableParallel({  #É uma função que recebe um dicionário de inputs e retorna um dicionário de outputs resumidamente\n",
    "    \"pergunta\": RunnablePassthrough(),\n",
    "    \"contexto\": retriever\n",
    "}).invoke(\"Como ser um samurai?\") # Passa a pergunta para o retriever e retorna o contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando uma alternativa não paralelizavel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dict = {\n",
    "    \"pergunta\": RunnablePassthrough(), \n",
    "    \"contexto\": retriever\n",
    "}\n",
    "chain = setup_dict | template | model | output_parser\n",
    "chain.invoke(\"Como ser um samurai?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando Fallback\n",
    "\n",
    "O fallback é um mecanismo de segurança que permite ter um \"plano B\" quando algo falha na execução da chain. Em outras palavras, é uma alternativa que entra em ação quando o componente principal não funciona como esperado.\n",
    "\n",
    "No contexto do LangChain, você pode usar fallbacks para:\n",
    "Modelos de linguagem (LLMs):\n",
    "Se um modelo falhar ou ficar indisponível, outro modelo pode ser usado como backup\n",
    "Por exemplo: se o GPT-4 falhar, pode usar o GPT-3.5 como fallback\n",
    "\n",
    "Embeddings:\n",
    "Se um serviço de embeddings falhar, outro pode ser usado\n",
    "Por exemplo: se OpenAI Embeddings falhar, pode usar HuggingFace Embeddings\n",
    "\n",
    "Retrievers:\n",
    "Se um método de recuperação falhar, outro pode ser usado\n",
    "Por exemplo: se a busca por similaridade falhar, pode usar busca por palavras-chave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "prompt = PromptTemplate.from_template(\"Resuma o seguinte texto: {texto}\")\n",
    "\n",
    "chain_pequena = prompt | llm\n",
    "chain_pequena.invoke({\"texto\":\"Oi, tudo bem?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='O texto é uma saudação informal que inicia uma conversa.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 279, 'prompt_tokens': 18, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_42bfad963b', 'finish_reason': 'stop', 'logprobs': None}, id='run-2c18a373-8845-421f-87e7-f2134fe21a22-0', usage_metadata={'input_tokens': 18, 'output_tokens': 279, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm1 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm2 = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "llm3 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm4 = ChatOpenAI(model=\"o3-mini-2025-01-31\")\n",
    "llm5 = ChatOpenAI(model=\"o1-2024-12-17\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Resuma o seguinte texto: {texto}\")\n",
    "\n",
    "chain_pequena = prompt | llm4\n",
    "chain_pequena.invoke({\"texto\":\"Oi, tudo bem?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_pequena.invoke({\"texto\":\"Oi, tudo bem?\" *1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oi, tudo bem?\\n\\nO texto traz uma repetição constante da mesma pergunta, \"Oi, tudo bem?\", sem nenhuma variação ou acréscimo de informações adicionais.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Resuma o seguinte texto: {texto}\")\n",
    "\n",
    "chain_grande = prompt | llm | StrOutputParser()\n",
    "chain_grande.invoke({\"texto\":\"Oi, tudo bem?\" *1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='O texto consiste na repetição contínua da saudação \"Oi, tudo bem?\", sem desenvolvimento de outras ideias ou informações adicionais.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 165, 'prompt_tokens': 5013, 'total_tokens': 5178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_42bfad963b', 'finish_reason': 'stop', 'logprobs': None}, id='run-323edc7c-7ecd-470f-b9c1-9df756b31c23-0', usage_metadata={'input_tokens': 5013, 'output_tokens': 165, 'total_tokens': 5178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_fallback = chain_pequena.with_fallbacks([chain_grande])\n",
    "chain_fallback.invoke({\"texto\":\"Oi, tudo bem?\" *1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fallback de Formatação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Qual foi a data do evento {evento} (no formato %Y-%m-%dT%H:%M:%S.%Z - Retorne apenas o valor solicitado)?\"\n",
    ")\n",
    "\n",
    "chain_instruct = prompt | OpenAI(model=\"gpt-3.5-turbo-instruct\") | DatetimeOutputParser()\n",
    "chain_instruct.invoke({\"evento\":\"Qual foi a data do final da copa de 2002?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_turbo = prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0125\") | DatetimeOutputParser()\n",
    "chain_turbo.invoke({\"evento\":\"Qual foi a data do final da copa de 2002?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_fallback = chain_instruct.with_fallbacks([chain_turbo])\n",
    "chain_fallback.invoke({\"evento\":\"Qual foi a data do final da copa de 2002?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dehbo\\AppData\\Local\\Temp\\ipykernel_14272\\221264837.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat_4o = ChatOpenAI(model=\"gpt-4o\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Parece que sua conexão está funcionando corretamente. Como posso ajudá-lo hoje?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 10, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_90d33c15d4', 'finish_reason': 'stop', 'logprobs': None} id='run-b57d7623-7e7e-4020-9d3f-ac0c28256350-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_4o = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "try:\n",
    "    response = chat_4o.invoke(\"Teste de conexão\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Erro direto na OpenAI: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
